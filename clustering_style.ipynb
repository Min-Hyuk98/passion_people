{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "c:\\users\\mks10\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:50: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "c:\\users\\mks10\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:50: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "c:\\users\\mks10\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:73: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "import csv\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "\n",
    "musinsa = pd.read_csv(\"musinsa_data_man.csv\")\n",
    "\n",
    "a = musinsa['top_tag']\n",
    "b = musinsa['bottom_tag']\n",
    "\n",
    "\n",
    "sentence = []\n",
    "for idx, i in enumerate(a):\n",
    "    sentence.append(a.loc[idx] + ', ' + b.loc[idx])\n",
    "\n",
    "musinsa['total_tag'] = sentence\n",
    "\n",
    "# 단어들의 벡터화\n",
    "model = Word2Vec(sentence, min_count=1, iter=200, size=300, sg=1)\n",
    "model.init_sims(replace=True)\n",
    "# print(\"섹시과 관련된 키워드 : \", model.most_similar(\"섹시\"))\n",
    "# print(\"스트리트와 관련된 키워드 : \", model.most_similar(\"스트리트\"))\n",
    "# print(\"심플와 관련된 키워드 : \", model.most_similar(\"심플\"))\n",
    "\n",
    "\n",
    "# ## 학습된 모델 사용시 벡터의 평균 점수 사용\n",
    "# •밑의 함수에 대한 요약 해석\n",
    "# : 한 사진마다 태그가 없으면 벡터화할 단어가 없으므로 0 값\n",
    "# : 단어가 있다면 태그 하나당 수치화(벡터화)의 수준을 300개로 할당\n",
    "#\n",
    "# -- ex) crop에 대한 점수(벡터)- 300개\n",
    "#\n",
    "# •결론: 각 단어마다 300개의 수치화된 점수로 구성된다.\n",
    "#\n",
    "\n",
    "\n",
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "#     항상 generate_missing if 안으로 들어감.\n",
    "#     각 단어들이 만든 모델에 있으면 vectorized 변수에 그 정보를 저장\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "sentence = pd.DataFrame(sentence, columns=None)\n",
    "\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = musinsa['total_tag'].apply(lambda x: get_average_word2vec(x, vectors, generate_missing=generate_missing))\n",
    "    return list(embeddings)\n",
    "\n",
    "training_embeddings = get_word2vec_embeddings(model, musinsa, generate_missing=True)\n",
    "\n",
    "\n",
    "# 모든 벡터에 대한 단어 저장\n",
    "vocabs = model.wv.vocab.keys()\n",
    "# print(vocabs)\n",
    "\n",
    "# 우리의 태그들에 대한 벡터\n",
    "word_vectors_list= [model[v] for v in vocabs]\n",
    "# print(word_vectors_list)\n",
    "\n",
    "# 태그들을 벡터화 시킨 값을 x에 저장\n",
    "# 2개의 성분으로 벡터들을 축소\n",
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(training_embeddings)\n",
    "\n",
    "# kmeans 클러스터링을 이용해 군집\n",
    "NUM_CLUSTERS=5\n",
    "kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)\n",
    "kmeans.fit(X)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "musinsa['class'] = labels\n",
    "\n",
    "musinsa.to_csv('clustering_style.csv',mode='w',index=False,encoding='utf-8')\n",
    "print(\"finish!!!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
